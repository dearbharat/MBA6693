[
["index.html", "MBA 6693 Data Analytics for Business Decisions Introduction Prerequisites Weekly Meeting Schedule Course Objectives Preclass checklist Course Material Schedule Evaluation Components Grade Determination Course Guidelines Academic Integrity Bharat Bhushan Verma Instructor", " MBA 6693 Data Analytics for Business Decisions Bharat Bhushan Verma 2020-03-27 Introduction This course focuses on developing skills necessary to perform data analytics processes and manage data lifecycle for business decision making. In this course we follow step by step process of data analytics including data extraction, transformation, analyzing and drawing inferences for business decisions. We will use MySQL for data extraction and preprocessing. We will analyze data using R to apply multivariate statistical learning techniques like Clustering, PCA, ARM and Logit models while also familiarising ourselves with version control using Git, interactive JuPyteR notebooks and RStudio. We will also learn advanced algorithms like Random Forest, kNN and SVM using R. Prerequisites MBA 6606 and MBA 6607 or ADM 2623/ADM 2624 and ADM 3628. Prior experience in statistics, optimization, databases and possibly some programming is highly desirable. Start learning GitHub on your own. You can ask doubts on Piazza Weekly Meeting Schedule Days Time Classroom Wednesday 19:00 to 21:50 SH351 Course Objectives Data Analytics is fast becoming a mainstream in business decision making. The objective of this course is to expose students to challenges and tools required in data acquisition from databases, preprocess and transform the data, apply relevant methods for drawing insights and convert the output into meaningful reports and visualizations for decision making. The purpose of data analytics will be to provide better consumer, market and operations insights for enhancing business value and automating business decisions. Course Tools This class is supported by DataCamp, the most intuitive learning platform for data science. Learn R, Python and SQL the way you learn best through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise. You will have access to the entire DataCamp course library for free for the duration of this course. I’ll make an announcement in Piazza detailing the effective dates. Piazza will be used for announcements, discussion, etc. R is the analytics tool we will be using. RStudio is the Integrated Development Environment (IDE) and the way you will be accessing R. RPubs is where we will be publishing our assignments and project. Preclass checklist By the end of first day of class, you are expected to: Sign up for Piazza. Make sure your name accurately reflects what is in UNB system. Accept the invite to DataCamp I emailed to you. I’ll be sending them to your maine.edu accounts the week before classes start and again on the first day of classes. DataCamp is free for the entire semester. Install R and RStudio on your computer. After you have completed all the items above, sign up for your free RPubs account. Install package manager Anaconda which provides support for R kernel in Jupyter Notebooks. Course Material Required Material [FDS] Elmasri Ramez, Navathe Shamkant Fundamentals of Database Systems 7th Edition ISBN: 9780136086208 [ISL] Trevor Hastie, Robert Tibshirani, Gareth James, Daniela Witten. An Introduction to Statistical Learning with Applications in R, Springer 978-1-4614-7137-0 [RGW] R for Data Science, Grolemund and Wickham Reference Material Most of the material will be on this site and in the DataCamp exercises. [RDS] R Programming for Data Science, Peng [YAR] YaRrr! The Pirate’s Guide to R, Phillips [GIT] Scott Chacon, Ben Straub. Pro Git, Second Edition, 978-1-4842-0077-3 Additional Optional Readings Schedule last updated: 2020-03-27 Week Chapter Topic Items_due Due Mon, Jan 06 FDS 4 Basic SQL DC - 1 Sat, Jan 11 Mon, Jan 13 FDS 5 More SQL DC - 2 Sat, Jan 18 Mon, Jan 20 RGW 3 Exploratory Data Analysis I DC - 3 Sat, Jan 25 Mon, Jan 27 RGW 4 Exploratory Data Analysis II DC - 4 Sat, Feb 01 Mon, Feb 03 ISL 3 Multiple Linear Regression DC - 5 Sat, Feb 08 Mon, Feb 10 ISL 4 StatsCan Data Science Competition DC - 6 Sat, Feb 15 Mon, Feb 17 ISL 4 Classification LDA &amp; Logit DC - 7 Sat, Feb 22 Mon, Feb 24 ISL 6 Model Selection &amp; Regularization DC - 8 Sat, Feb 29 Mon, Mar 09 ISL 10 PCA and Clustering DC - 9 Sat, Mar 14 Mon, Mar 16 ISL 8 Tree Based Models DC - 10 Sat, Mar 21 Mon, Mar 23 ISL 8 Random Forest, Bagging &amp; Boosting DC - 11 Sat, Mar 28 Mon, Mar 30 ISL 9 SVM DC - 12 Sat, Apr 04 Mon, Apr 06 Course Wrap Up NA Thu, Apr 09 Evaluation Components Table 0.1: Evaluation components and weightages. Evaluation Weightage Date Data Camp (12) 48 Check Schedule Test 01 5 January 28 Test 02 5 February 25 Class Participation 12 Weekly Submissions Individual Project 10 March 10 Group Project 20 March 31 Note: DC - Data Camp Exercises Class participation @ 1% is given for timely weekly submissions. A delay of week is allowed with a penalty of 1% class participation component Any submission delayed beyond a week will result in loss of 5% weightage. You have an additional bonus component of 5% awarded for class participation. This will be solely judged by your participation on your contributions on Piazza. You must ask at least five questions or answer questions on Piazza to receive full credit. The credit shall be granted at the sole discretion of the instructor. Grade Determination Table 0.2: Grade Conversion. Marks Grade Marks Grade Marks Grade 89.51 - 100 A+ 84.51 - 89.5 A 79.51 - 84.5 A- 74.51 - 79.5 B+ 69.51 - 74.5 B 64.51 - 69.5 B- 59.51 - 64.5 C+ 54.51 - 59.5 C 49.51 - 54.5 D 0 - 49.5 F Course Guidelines There will be some practice sets released from time to time, which you can submit for feedback, but those will not be graded. However, the questions in assignments will be similar to the assignments. Therefore finishing these assignments independently will not only be very helpful in preparing for all examinations, but also enhance your understanding of the materials. Each quiz will be of twenty (30) minutes duration starting from the beginning of each designated class. There is no final examination but the two term tests will be cover all the material taught until the day of examination. All exams and quizzes are closed books and notes. A formula sheet, exactly the same as the one at the end of this course outline, will be provided by the instructor for all examinations. No self-prepared formula sheet is allowed. The exams may be conducted online in class or through a 24 - 48 hours take home submissions. Students who miss quizzes for whatever reasons will have the weights of the missed quizzes carried on to the term examinations. Questions and problems in all exams will be based on material discussed in the class. It is your responsibility to attend class regularly to get all the information related to the course throughout this term. Please note that attendance is expected and a student is unlikely to complete the course with success if absent. Candidates will turn off cell phones, pagers, blackberries, or other electronic devices for the duration of the class. A special request for rewriting examination may be considered only for term tests but the latest performance will be considered as final performance. So a retake may increase or decrease the marks obtained. Academic Integrity The University of New Brunswick places a high value on academic integrity and has a policy on plagiarism, cheating and other academic offences. Plagiarism includes: 1) quoting verbatim or almost verbatim from any source, including all electronic sources, without acknowledgement; 2) adopting someone else’s line of thought, argument, arrangement, or supporting evidence without acknowledgement; 3) submitting someone else’s work, in whatever form, without acknowledgement; and 4) knowingly representing as one’s own work any idea of another. Examples of other academic offences include: 1) cheating on exams, tests, assignments or reports; 2) impersonating somebody at a test or exam; 3) obtaining an exam, test or other course materials through theft, collusion, purchase or other improper manner; 4) submitting course work that is identical or substantially similar to work that has been submitted for another course; and 5) more, as set out in the academic regulations found in the Undergraduate Calendar (https://eservices.unb.ca/calendar/undergraduate/). Penalties for plagiarism and other academic offences range from a minimum of F (zero) in the assignment, exam or test to a maximum of suspension or expulsion from the University, plus a notation of the academic offence on the student’s transcript. For more information, please see the Undergraduate Calendar, Section B, Regulation VIII.A (https://eservices.unb.ca/calendar/undergraduate/display.cgi?tables=regulationsSubLevel1&amp;id=43) or visit: http://nocheating.unb.ca. It is the student’s responsibility to know the Regulations. Bharat Bhushan Verma Instructor I strongly urge you to post your academic queries on Piazza. If you are shy and do not want to reveal your identity while posting your query, you can post anonymously. Similarly, I strongly urge you to answer the question posted by other students so that you can test your own understanding. While doing so you can earn the bonus 5% extra credits. It can make a difference to your getting your scholarship or not. Take full advantage of the Piazza platform to maximize your score. I am restricting myself to answer posted queries for eight hours to give enough room for other students to reply to the query. I won’t be answering any queries related to the course sent to my email. I will only respond to queries posted on Piazza only. You can contact me through email on bharat.verma@unb.ca for anything that is specific to you or only few of you. You are welcome to meet me in my office 321, Tilley Hall with prior appointment between Monday and Wednesday only. "],
["getting-started.html", "Week 1 Getting Started 1.1 Media 1.2 DataCamp Exercises 1.3 Tools and Methods", " Week 1 Getting Started This unit spans Mon, Jan 06, 2020 through Sat, Jan 11, 2020. At 11:59 PM on Sat, Jan 11, 2020 the following items are due: Introduction to SQL or Intermediate SQL This unit reviews the course syllabus, expectations, and the tools we will be using throughout the semester. We’ll also cover the broad overview of the data analytics lifecycle. We’ll also introduce some new terms and their definitions in this unit. 1.1 Media I shall upload recorded videos of important topics and concepts for referral in due course of time as required and found necessary. There will be PPTs as well for reference. 1.1.1 Why Piazza? In my opinion, Piazza is one of the better ways to foster community among students using online and offline blended learning class. We’ll be using it: for class announcement for private and public messaging to post questions and help answer them In Piazza, sign up for for this course at Piazza. Piazza also has a mobile application for Android, and iOS. 1.2 DataCamp Exercises This unit’s DataCamp exercises start you off with the Introduction to SQL course. Those who are well versed with SQL can instead do Intermediate SQL. 1.3 Tools and Methods Enabling Tools for Data Analytics "],
["more-sql.html", "Week 2 More SQL 2.1 Media 2.2 DataCamp Exercises 2.3 What is SQL Anyway", " Week 2 More SQL This unit spans Mon, Jan 13, 2020 through Sat, Jan 18, 2020. At 11:59 PM on Sat, Jan 18, 2020 the following items are due: Joining Data in SQL or Database Design Unit 2 is a continuation to the first unit. Here we will master SQL commands. We will have lesser load in Unit 3. We will start building up the pace again through unit 4 - 5. At that point, things should start to smooth out a bit. We’ll then have covered enough SQL and exploratory data analysis in R to take in a data set, clean it, and present it visually. 2.1 Media I shall upload recorded videos of important topics and concepts for referral in due course of time as required and found necessary. There will be PPTs as well for reference. 2.2 DataCamp Exercises This unit’s DataCamp exercises have you finish basics required in SQL, which gives you the prerequisite knowledge required for many of the other courses. 2.3 What is SQL Anyway Database Technologies Landscape "],
["sql-to-r.html", "Week 3 SQL to R 3.1 Media 3.2 Data Management 3.3 dplyr 3.4 ggplot2 3.5 Grammar of graphics 3.6 dplyr window functions 3.7 Perception 3.8 Planning a Visualization 3.9 DataCamp Exercises 3.10 Credits 3.11 DataCamp Exercises 3.12 What is R?", " Week 3 SQL to R This unit spans Mon, Jan 20, 2020 through Sat, Jan 25, 2020. At 11:59 PM on Sat, Jan 25, 2020 the following items are due: Introduction to the Tidyverse or Manipulating Dataframes in Pandas Please do not start the assigned datacamp tutorials until class on Tuesday January 21 so that you have a good foundation to attempt the tutorial assignment. Now that we’ve learnt SQL and communication with RDBMS, we will cover how we extract data from SQL into R environment. We will learn some fundamental building blocks of R in the classroom. Besides it is time to introduce some packages as well which will make our life easier when manipulating and exploring data. We’ll also use external packages to visually explore our data (i.e., create simple visualizations). 3.1 Media 3.1.1 Suggested Readings For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights Markdown R-FAQ 2.10 - What is CRAN? R Packages: A Beginner’s Guide What is the tidyverse?, by Joseph Rickert{target=’_blank} 3.1.2 Videos Video: Installing SQL Workbench Video: Connect to SQL from Workbench Video: SQL 2 CSV Video: CSV to SQL Video: SQL 2 R 3.1.3 Data for SQL Exercise CSV files for SQL Exercise Rscript for SQL2R 3.2 Data Management The NY Times article describes what they aptly termed “janitorial” work as data wrangling. The other often used term is data munging and it always involves getting data from some raw format into something more usable. How much work this requires depends on a few factors including the completeness, correctness, structure and source/type of data. Completeness and correctness are what they sound like. Is there missing data, and is the data that we have correct? Assuming we have some missing data, we have a couple of options – we can discard the missing data, or impute (i.e., guess) what the missing values might be. We usually want to find out if our data is complete and correct before we start doing any analysis on it. The process of correcting data is often referred to as data cleansing or scrubbing. Structure is a little more abstract concept. Highly structured data is usually the easiest to work with. It is often what we see in a relational database where we have fields like start_date, last_name, hourly_wage that can only store acceptable values. Items you find in a spreadsheet are also highly structured, albeit typically less structured than a relational database. Unstructured data are pretty much the opposite but data isn’t usually completely unstructured. Most of what we consider to be unstructured data could be classified as semi-structured. If we examine the tweet above, there are some very structured data. @TheEllenShow is the account name, #AppleLive is a categorical label we can place on a tweet called a hashtag. The time, date, count of retweets and likes are also examples of structured data. The actual text of the tweet is where we have significantly less structure. It is also the hardest for machines to interpret or imply meaning. Product companies often analyze tweets as a way of viewing trends in product sentiment. At first glance “so excited” seems like a very positive comment. As a human, we don’t have to work too hard to figure out this tweet is sarcastic and isn’t definitively reflecting a positive sentiment. The source/type of data often acts as a rough indicator of how much data wrangling we are going to need to do. For example, if I was starting an analytics project and you told me all of the data came from an internal corporate reporting database, I would be at least somewhat confident that the amount of preparation for analysis wouldn’t be overwhelming. If you said, however, that the data came from a bunch of legal briefs (as pdf files) and phone calls (as audio files), my expectations of preparation time would change dramatically. In this course, we will be mostly working with and cleaning structured data but exposing you to the process and concept of tidying data, which applies to both structured and unstructured data. All analytics projects have an associated workflow with them. The tools we will be using in this class support the tidy workflow described in R for Data Science and shown below. While this course will cover all stages of the workflow, we will be spending considerable amounts of time: importing data from a variety of sources; tidying our data to a consistent format; transforming the data to conform with what we want to analyze and; communicating results. Your first assignment, due at the end of this unit, has you focusing on communications by publishing a simple report about yourself. Before we get to explore any of these stages, we need to learn a commonly used tool for analytics, namely R. ## The tidyverse The tidyverse is a set of external R packages that work together and support the analytics workflow we introduced in the first unit. It was originally casually referred to as the “hadleyverse” as the lead developer on all the initial packages was Hadley Wickham, but he preferred to refer to these packages as the tidyverse explicitly. There are many packages in the tidyverse, but for this unit, we will be covering two of the packages that are automatically loaded when you issue the command library(tidyverse), namely dplyr for data manipulation and ggplot2 for visualization. If you like to follow along with RStudio while you read these notes, make sure you install the tidyverse package (RStudio –&gt; Tools –&gt; Install Package –&gt; tidyverse) and load the library. Before we get started with dplyr, it is important to mention that all of the tidyverse package support the pipe operator %&gt;% which is used to chain together statements and is technically part of the tidyverse package magrittr. We’ll start out this unit by loading the core packages in the tidyverse. library(tidyverse) 3.3 dplyr dplyr is a grammar for data manipulation that uses specific verbs to manipulate data. it is often difficult to get used to asking questions in dplyr instead of plain English. One way to help improve your thought process is to understand the verbs of dplyr and their purpose. select chooses specific columns. rename renames specific columns and selects all. filter chooses specific rows. arrange sorts rows. mutate creates new columns. transmute is like mutate but doesn’t keep your old columns. distinct returns unique rows. summarize aggregates or chunks. slice selects rows by position. sample takes samples of data (seldom used). We won’t be discussing sample as it is more commonly used in the sciences, but the other verbs are all commonly used. The other two key non-verb actions in dplyr are group_by, which is typically applied when using summarize and the pipe operator %&gt;% which is used to combine verbs. I give a better visual representation of the queries below in the video, but let’s start by reading in a comma separated values (csv) file from a url and having a quick look at it. Note: we will learn all about importing different filetypes later in the semester. got &lt;- read.csv(&quot;./data/got.csv&quot;, stringsAsFactors = FALSE) got ## lastname firstname major year gpa ## 1 Snow John Nordic Studies Junior 3.23 ## 2 Lannister Tyrion Communications Sophomore 3.83 ## 3 Targaryen Daenerys Zoology Freshman 3.36 ## 4 Bolton Ramsay Phys Ed Freshman 2.24 ## 5 Stark Eddard History Senior 2.78 ## 6 Clegane Gregor Phys Ed Sophomore 3.23 ## 7 Baelish Peter Communications Freshman 2.84 ## 8 Baratheon Joffrey History Freshman 1.87 ## 9 Drogo Khal Zoology Senior 3.38 ## 10 Tarly Samwise Nordic Studies Freshman 2.39 got.csv is read into the data frame got using read.csv. We’ll use the pipe operator %&gt;% to pipe the data frame the select verb to choose specific columns (e.g., lastname, firstname, gpa). Within select, I can also change column names. Please note: I am not storing the results of these queries in any variables…I am sending them directly out to output (i.e., printing them out). Below, we are explicitly saying “take the data frame got and select the columns lastname, firstname, and gpa and while you are at it, rename the lastname column to surname.” got %&gt;% select(surname = lastname, firstname, gpa) ## surname firstname gpa ## 1 Snow John 3.23 ## 2 Lannister Tyrion 3.83 ## 3 Targaryen Daenerys 3.36 ## 4 Bolton Ramsay 2.24 ## 5 Stark Eddard 2.78 ## 6 Clegane Gregor 3.23 ## 7 Baelish Peter 2.84 ## 8 Baratheon Joffrey 1.87 ## 9 Drogo Khal 3.38 ## 10 Tarly Samwise 2.39 We can use rename to change column names…it selects all the columns in the data frame. So if I wanted to show the entire data frame using the more formal surname instead of lastname, I could do the following without having to specify all of the names in select. got %&gt;% rename(surname = lastname) ## surname firstname major year gpa ## 1 Snow John Nordic Studies Junior 3.23 ## 2 Lannister Tyrion Communications Sophomore 3.83 ## 3 Targaryen Daenerys Zoology Freshman 3.36 ## 4 Bolton Ramsay Phys Ed Freshman 2.24 ## 5 Stark Eddard History Senior 2.78 ## 6 Clegane Gregor Phys Ed Sophomore 3.23 ## 7 Baelish Peter Communications Freshman 2.84 ## 8 Baratheon Joffrey History Freshman 1.87 ## 9 Drogo Khal Zoology Senior 3.38 ## 10 Tarly Samwise Nordic Studies Freshman 2.39 If I wanted to filter the results above to show gpa’s that are greater than or equal to 3.5, I would pipe the results to filter to choose those specific rows. got %&gt;% rename(surname = lastname) %&gt;% filter(gpa &gt;= 3.5) ## surname firstname major year gpa ## 1 Lannister Tyrion Communications Sophomore 3.83 If instead, I just wanted to sort the selected columns from highest to lowest gpa, I would use arrange. I use desc because the default sort order is lowest to highest. got %&gt;% rename(surname = lastname) %&gt;% arrange(desc(gpa)) ## surname firstname major year gpa ## 1 Lannister Tyrion Communications Sophomore 3.83 ## 2 Drogo Khal Zoology Senior 3.38 ## 3 Targaryen Daenerys Zoology Freshman 3.36 ## 4 Snow John Nordic Studies Junior 3.23 ## 5 Clegane Gregor Phys Ed Sophomore 3.23 ## 6 Baelish Peter Communications Freshman 2.84 ## 7 Stark Eddard History Senior 2.78 ## 8 Tarly Samwise Nordic Studies Freshman 2.39 ## 9 Bolton Ramsay Phys Ed Freshman 2.24 ## 10 Baratheon Joffrey History Freshman 1.87 Suppose I wanted to create a dean’s list column called dlist and set it to TRUE if the gpa &gt;= 3.5 and FALSE otherwise. I would use mutate for that. Note: in this example, the column is only created in the output, and the data frame is unaltered. got %&gt;% rename(surname = lastname) %&gt;% mutate(dlist = gpa &gt;= 3.5) ## surname firstname major year gpa dlist ## 1 Snow John Nordic Studies Junior 3.23 FALSE ## 2 Lannister Tyrion Communications Sophomore 3.83 TRUE ## 3 Targaryen Daenerys Zoology Freshman 3.36 FALSE ## 4 Bolton Ramsay Phys Ed Freshman 2.24 FALSE ## 5 Stark Eddard History Senior 2.78 FALSE ## 6 Clegane Gregor Phys Ed Sophomore 3.23 FALSE ## 7 Baelish Peter Communications Freshman 2.84 FALSE ## 8 Baratheon Joffrey History Freshman 1.87 FALSE ## 9 Drogo Khal Zoology Senior 3.38 FALSE ## 10 Tarly Samwise Nordic Studies Freshman 2.39 FALSE If I just wanted to show my transformed variables and no other variables, I could use transmute got %&gt;% transmute(name = paste(firstname, lastname), dlist = gpa &gt;= 3.5) ## name dlist ## 1 John Snow FALSE ## 2 Tyrion Lannister TRUE ## 3 Daenerys Targaryen FALSE ## 4 Ramsay Bolton FALSE ## 5 Eddard Stark FALSE ## 6 Gregor Clegane FALSE ## 7 Peter Baelish FALSE ## 8 Joffrey Baratheon FALSE ## 9 Khal Drogo FALSE ## 10 Samwise Tarly FALSE If we wanted to list the majors represented in the got data frame, we would use distinct, which restricts to unique(distinct) output. got %&gt;% distinct(major) ## major ## 1 Nordic Studies ## 2 Communications ## 3 Zoology ## 4 Phys Ed ## 5 History Aggregation often adds the most complexity to a query, and it is quite common to see summarize combined with group_by. For example, if we wanted to show the average gpa for each major, we would use group_by to declare that we are doing a calculation for each major and use summarize to define the mean calculation. You’ll notice that instead of a data frame, we are outputting a tibble, which is essentially an enhanced data frame that can store more complex data. got %&gt;% group_by(major) %&gt;% summarize(average_gpa = mean(gpa)) ## # A tibble: 5 x 2 ## major average_gpa ## &lt;chr&gt; &lt;dbl&gt; ## 1 Communications 3.34 ## 2 History 2.33 ## 3 Nordic Studies 2.81 ## 4 Phys Ed 2.74 ## 5 Zoology 3.37 Suppose we wanted to show the name of the student with the highest gpa for each major. We could do this in a few different ways. In all cases, since we are doing it for each major, we will be using group_by(major). In the first case, after grouping, we sort in descending gpa order and slice out the first(1) instance of each student. got %&gt;% group_by(major) %&gt;% arrange(desc(gpa)) %&gt;% slice(1) ## # A tibble: 5 x 5 ## # Groups: major [5] ## lastname firstname major year gpa ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Lannister Tyrion Communications Sophomore 3.83 ## 2 Stark Eddard History Senior 2.78 ## 3 Snow John Nordic Studies Junior 3.23 ## 4 Clegane Gregor Phys Ed Sophomore 3.23 ## 5 Drogo Khal Zoology Senior 3.38 In the second case, we decide we want to use the top_n function. got %&gt;% group_by(major) %&gt;% arrange(desc(gpa)) %&gt;% top_n(1) ## Selecting by gpa ## # A tibble: 5 x 5 ## # Groups: major [5] ## lastname firstname major year gpa ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Lannister Tyrion Communications Sophomore 3.83 ## 2 Drogo Khal Zoology Senior 3.38 ## 3 Snow John Nordic Studies Junior 3.23 ## 4 Clegane Gregor Phys Ed Sophomore 3.23 ## 5 Stark Eddard History Senior 2.78 In the third case, we use the min_rank function within filter. got %&gt;% group_by(major) %&gt;% filter(min_rank(desc(gpa)) == 1) ## # A tibble: 5 x 5 ## # Groups: major [5] ## lastname firstname major year gpa ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Snow John Nordic Studies Junior 3.23 ## 2 Lannister Tyrion Communications Sophomore 3.83 ## 3 Stark Eddard History Senior 2.78 ## 4 Clegane Gregor Phys Ed Sophomore 3.23 ## 5 Drogo Khal Zoology Senior 3.38 This should seem somewhat confusing, and perhaps it is best to describe what is going on here. top_n is an easier to use “wrapper” function that combines filter and min_rank. slice was added later to dplyr to make it simpler not just to select the top. For example, if I wanted to select positions 2 through 4, I would use slice(2:4) There is no equivalent top_n for this, and I would end up resorting to the harder to follow filter(min_rank(...) %in c(2:4) To simplify, you should try to get comfortable with slice but feel free to use top_n as well. 3.4 ggplot2 We aren’t wired to look at tons of numbers. In analytics, we tend to use visualizations to understand our data and observe patterns quickly. One of R’s primary strengths is its visualization libraries. For static visualizations, ggplot2 is possibly the most commonly used library. We often think of visualizations as a way to tell stories to others involving data. In this case, we are merely using visualization to explore our data. With exploratory visualizations, we aren’t that focused on formatting and ease of interpretation by others because they are for our private consumption. As we’ll learn this semester, ggplot2 is commonly used for both exploratory visualizations and to communicate results. 3.5 Grammar of graphics Base R graphics are conceptually like working from a blank canvas. If you’ve used Microsoft Excel to create a visualization, you typically select a chart from a library. Leland Wilkinson published The Grammar of Graphics in 1999 and described a framework for constructing visualizations. This structured framework falls nicely in between the unstructured blank canvas and the rigid “select a chart” model. The “gg” in ggplot2 actually stands for grammar of graphics. Another commonly used visualization software application, Tableau, also uses the grammar of graphics as a framework (Leland Wilkinson was the VP of Statistics for Tableau). It is important to note that this grammar doesn’t help you select what visualizations to use, it merely helps you construct them. There are three critical components for every ggplot2 plot. data a set of aesthetic mappings between variables in the data and visual properties, and at least one layer which describes how to render each observation. Layers are usually created with a geom function. First, we’ll take a look at a base graphics scatterplot of miles per gallon(mpg) and displacement(disp) using the built-in mtcars data frame: plot(mtcars$mpg, mtcars$disp) If I want to create a similar visualization in ggplot2, I would start with the data and the aesthetics (aes) using the ggplot function. ggplot(data = mtcars, aes(x = mpg, y = disp)) You’ll notice that there aren’t any points in our graph. That is because we have yet to create a layer to render the observations. Recall that we typically use a geom function for this. Scatterplots are rendered using geom_point. ggplot(data = mtcars, aes(x = mpg, y = disp)) + geom_point() Looking back at the layered grammar we created: data - mtcars aesthetics - we map mpg to the x-axis and disp to the y-axis layer - we use points as the geometric object to render the values You’ll probably notice that the visualization is a little more refined than the one we created with plot. One of the benefits of using ggplot2 is that the defaults are really good. We can also apply other aesthetic mappings to our visualization, like mapping cylinder to color: ggplot(data = mtcars, aes(x = mpg, y = disp, color = factor(cyl))) + geom_point() I used factor to effectively treat displacement as a factor (i.e., enumerated or categorical) variable. This creates a potentially more diverging color scheme and prevents a legend that might include values that don’t exist in the data (shown below without the use of factor). ggplot(data = mtcars, aes(x = mpg, y = disp, color = cyl)) + geom_point() Some other aesthetic mappings include size: ggplot(data = mtcars, aes(x = mpg, y = disp, color = factor(cyl), size = hp)) + geom_point() …and shape: ggplot(data = mtcars, aes(x = mpg, y = disp, color = factor(cyl), size = hp, shape = factor(gear))) + geom_point() The aesthetics we use are somewhat dependent on how we choose to encode our data. Some aesthetics not used or not applicable here are fill, linetype, weight, alpha, and text. The visualization above is somewhat difficult to comprehend, and we might be better off rethinking what data we want to show and how we want to communicate it. There are also a variety of geoms for bars, boxplots, smoothing lines, and others that you can use, some of which we will cover thoughout the semester. The remaining grammatical elements that we have yet to cover, are: The scales map values in the data space to values in an aesthetic space, whether it be color, or size, or shape. Scales draw a legend or axes, which provide an inverse mapping to make it possible to read the original data values from the plot. A coordinate system, coord for short, describes how data coordinates are mapped to the plane of the graphic. It also provides axes and gridlines to make it possible to read the graph. We normally use a Cartesian coordinate system, but some others are available, including polar coordinates and map projections. A faceting specification describes how to break up the data into subsets and how to display those subsets as small multiples. This is also known as conditioning or latticing/trellising. A theme which controls the finer points of display, like the font size and background colour. While the defaults in ggplot2 have been chosen with care, you may need to consult other references to create an attractive plot. A good starting place is Edward Tufte’s early works (Tufte, 1990, 1997, 2001). 3.6 dplyr window functions As defined in the dplyr documentation, a window function is a variation on an aggregation function. Where an aggregation function, like sum() and mean(), takes n inputs and return a single value, a window function returns n values. The window functions we’ll be dealing with in this class are often ranking functions (like min_rank()) and offset functions (like lag() which was introduced in the last unit). If you have ever worked with relational databases, window functions are commonly implemented in SQL. The ranking and ordering functions you may use in dplyr are: row_number() min_rank() which allows for gaps in ranks (e.g., if two rows are tied for first, the next rank is third) dense_rank() which doesn’t allow for gaps in ranks (e.g., if two rows are tied for first, the next rank is second) percent_rank() a number between 0 and 1 computed by rescaling min_rank to [0, 1]. cume_dist() a cumulative distribution function. Proportion of all values less than or equal to the current rank. ntile() a rough rank, which breaks the input vector into n buckets If you look at a ranking of gpa’s in the got data, 3.23 is tied for fourth place, and 2.84, which is the 6th row in the arranged data frame would be in sixth place using min_rank(), fifth place, using dense_rank() got &lt;- read.csv(url(&quot;http://jamessuleiman.com/teaching/datasets/got.csv&quot;), stringsAsFactors = FALSE) got %&gt;% filter(row_number(desc(gpa)) == 6) ## lastname firstname major year gpa ## 1 Baelish Peter Communications Freshman 2.84 got %&gt;% filter(min_rank(desc(gpa)) == 6) ## lastname firstname major year gpa ## 1 Baelish Peter Communications Freshman 2.84 got %&gt;% filter(dense_rank(desc(gpa)) == 5) ## lastname firstname major year gpa ## 1 Baelish Peter Communications Freshman 2.84 We could also use the slice verb to accomplish the same thing. got %&gt;% arrange(desc(gpa)) %&gt;% slice(6) ## lastname firstname major year gpa ## 1 Baelish Peter Communications Freshman 2.84 We’ll add the columns p_rank, c_dist and ntile to show you how the remaining ranking functions work. We’ll use four buckets for ntile() got %&gt;% select(lastname, firstname, gpa) %&gt;% arrange(desc(gpa)) %&gt;% mutate(p_rank = percent_rank(gpa), cdist = cume_dist(gpa), ntile = ntile(gpa, 4)) ## lastname firstname gpa p_rank cdist ntile ## 1 Lannister Tyrion 3.83 1.0000000 1.0 4 ## 2 Drogo Khal 3.38 0.8888889 0.9 4 ## 3 Targaryen Daenerys 3.36 0.7777778 0.8 3 ## 4 Snow John 3.23 0.5555556 0.7 3 ## 5 Clegane Gregor 3.23 0.5555556 0.7 3 ## 6 Baelish Peter 2.84 0.4444444 0.5 2 ## 7 Stark Eddard 2.78 0.3333333 0.4 2 ## 8 Tarly Samwise 2.39 0.2222222 0.3 1 ## 9 Bolton Ramsay 2.24 0.1111111 0.2 1 ## 10 Baratheon Joffrey 1.87 0.0000000 0.1 1 The offset functions you may use in dplyr are: lag() returns the previous value in the vector - introduced in the last unit. lead() returns the next value in a vector - the opposite of lag() If we wanted to know the gpa of the next better lag() and next worst lead() students I would use: got %&gt;% arrange(desc(gpa)) %&gt;% mutate(nxt_better = lag(gpa), nxt_worst = lead(gpa)) ## lastname firstname major year gpa nxt_better nxt_worst ## 1 Lannister Tyrion Communications Sophomore 3.83 NA 3.38 ## 2 Drogo Khal Zoology Senior 3.38 3.83 3.36 ## 3 Targaryen Daenerys Zoology Freshman 3.36 3.38 3.23 ## 4 Snow John Nordic Studies Junior 3.23 3.36 3.23 ## 5 Clegane Gregor Phys Ed Sophomore 3.23 3.23 2.84 ## 6 Baelish Peter Communications Freshman 2.84 3.23 2.78 ## 7 Stark Eddard History Senior 2.78 2.84 2.39 ## 8 Tarly Samwise Nordic Studies Freshman 2.39 2.78 2.24 ## 9 Bolton Ramsay Phys Ed Freshman 2.24 2.39 1.87 ## 10 Baratheon Joffrey History Freshman 1.87 2.24 NA We’ve covered a good portion of dplyr and most of what you’ll be using for the remainder of the semester. 3.7 Perception Colin Ware, a professor at UNH, covers perception in great detail – in both of his books. It is relatively easy to follow visual design heuristics like “use high contrast,” and learning some rules and guidelines for constructing visualizations will go a long way to improve your skills at creating good visualizations. Understanding human visual perception takes a great deal more work but will also enhance your ability to ascertain a certain level of mastery in creating visualizations. With regards to high contrast, if we see the image below, the lion’s sand color is not in high contrast to the greenish hues of the tall grasses, yet we can spot the lion quite easily. We are genetically hardwired to see the lion as our genetic ancestry mostly doesn’t include people that could not see the lion - they were eaten. Creative Commons licensed, Flickr user Heather Bradley Before we get too far into why we so readily see the lion and how that relates to creating good visualizations, it is essential to understand that some graphics are well understood because they are part of our visual language and are more similar to words on a page. A graphic like the one shown below would be an excellent example of this. I’ve removed the legend. Take a second and see if you can guess what this graphic is showing? NOAA Weather Map If you guessed that this is a temperature map for the United States, you would be correct. The reason you were able to guess what the map was is that you have seen it before. It is part of your learned language. If graphical perception was purely based on learned graphical conventions, understanding human visual perception would not be necessary in creating visualizations. One would merely spend time learning the conventions. Conventions are relevant, however observing the lion in the tall grass isn’t part of a learned language - it is sensory. As shown in the neuroscience video with Scott Murray, explaining visual perception to the layperson, with no background in neuroscience, is difficult. Here are the simplified steps he describes in the video: Light enters our eye. Gets transduced (i.e., converted from light signals to neural signals) by our retina into visual information. Visual information travels to the cortex. Stops in the lateral geniculate nucleus in the thalamus. Projects directly to the cortex in an area called V1 or primary visual cortex. V1 to other cortical regions (e.g., V2, V3, parietal cortex, temporal lobe, etc.). There are upwards of 30 different visual areas in the brain. Perception is a complex interaction that isn’t fully understood. It also depends on what we are processing. For example, motion is processed differently than color. Sounds simple, right? Visual perception is an attempt by our brains to figure out what caused a pattern on our retina. In that process, the brain tries to prioritize what it thinks is important (e.g., the lion in the grass). This importance filtering is referred to as pre-attention. Look at the pattern below. Can you count how many times the number 5 appears in the list? 13029302938203928302938203858293 10293820938205929382092305029309 39283029209502930293920359203920 You had to attentively process the entire list to count the number of 5’s. This probably took quite a bit of time. Try counting again using the list below. 130293029382039283029382038582931029382093820592938209230502930939283029209502930293920359203920 That was quite a bit easier and illustrative of preattentive processing. We told your brain what was important by using shading or color intensity. Many visual features have been identified as preattentive. Christopher G. Healy summarizes them very well in the table below copied from his site on perception in visualization. On Healy’s table, he also lists the citations for the psychology studies that examined each visual feature. line (blob) orientation length, width closure size curvature density, contrast number, estimation colour (hue) intensity, binocular lustre intersection terminators 3D depth cues, stereoscopic depth flicker direction of motion velocity of motion lighting direction 3D orientation artistic properties Table 1: A partial list of preattentive visual features. So how does this explain our rapid identification of the lion in the tall grass? The explanation is probably quite a bit more complicated than the observable pattern shifts between the lion and her surroundings. As humans, we probably tend first to look where things might be hiding. Nonetheless, the volumes of human visual perception research help us provide some guidelines and considerations when preparing graphics. My favorite synthesis of best uses of visual encodings is this chart, compiled by Noah Iliinksy. He gives simple guidelines for selecting visual encodings depending on the type of data you have (i.e., quantitative, ordinal, categorical, relational). Don’t think of this as hard rules. It is more like suggested guidance in selecting visual encodings. For example, NOAA does use color to represent quantitative data (temperature) even though it is not recommended. Since the use of color in weather maps is so familiar, it has become part of our visual vocabulary and is not only considered acceptable but preferred. 3.8 Planning a Visualization Generally speaking, the starting point for planning a visualization is looking at the data. We typically want to get the data in a tidy format first. We’ll use a local dataset from data.maine.gov – Maine population by county (per decade, 1960-2010). We’ll assume I don’t have a specific question that I’m trying to answer; I want to see what might be interesting. These could include: counties with abnormal growth rates (high or low) shifts in population over time etc. I’ve already downloaded and cleaned up the data to save you the trouble of going to data.maine.gov and downloading and tidying the original data. county_pop &lt;- read.csv(url(&quot;http://jamessuleiman.com/teaching/datasets/maine_county_population.csv&quot;), stringsAsFactors = FALSE) str(county_pop) ## &#39;data.frame&#39;: 96 obs. of 3 variables: ## $ county : chr &quot;Androscoggin&quot; &quot;Aroostook&quot; &quot;Cumberland&quot; &quot;Franklin&quot; ... ## $ year : int 1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ... ## $ population: int 86312 106064 182751 20069 32293 89150 28575 18497 44345 126346 ... I want to quickly see if there is a growth rate story to be told, so I’ll use ggplot2 to make small multiples for exploratory visualization. I’ll be doing one chart for each county using facets, which have their specification in the layered grammar. We’ll cover facets in more detail later in the semester. I’ll do a county by county comparison, so my aesthetics will be: x = year y = population Because I’m interested in the growth rate, I’ll use geom_line and geom_point. ggplot(county_pop, aes(x = year, y = population)) + geom_line() + geom_point() + facet_wrap(~ county) It looks like Southern Maine grew, while Northern Maine did not. That would be a compelling story to tell on a map, but for this example, since I’m not using a map I’m going to compare the two most southeastern counties (York and Cumberland) to the two most northeastern counties. I’ll still use the same aesthetics and geoms, but I’ll change county from a facet to an aesthetic – color. ggplot(county_pop %&gt;% filter(county %in% c(&quot;Aroostook&quot;, &quot;Washington&quot;, &quot;York&quot;, &quot;Cumberland&quot;)), aes(x = year, y = population, col = county)) + geom_line() + geom_point() Finally, we’ll change the breaks on the y-axis to increments of 50,000 and use a red-green colorblind friendly palette. We’ll use the simple scale_color_colorblind() function from package ggthemes. You’ll notice that the color differences aren’t as easy to notice as the default ggplot2 palette. If we wanted more control or options over our colorblind palette, we could use the dichromat package or the colorblind friendly palettes from colorbrewer.org. Keep in mind that there are also computer tools for colorblind users that automatically transform colors on websites. library(ggthemes) ggplot(county_pop %&gt;% filter(county %in% c(&quot;Aroostook&quot;, &quot;Washington&quot;, &quot;York&quot;, &quot;Cumberland&quot;)), aes(x = year, y = population, col = county)) + geom_line() + geom_point() + scale_y_continuous(breaks = c(50000, 100000, 150000, 200000, 250000)) + scale_color_colorblind() 3.9 DataCamp Exercises The DataCamp exercises due this unit continue coverage of dplyr and ggplot2. These exercises will complete the DataCamp “Introduction to the Tidyverse” course. DataCamp also provides certificates for these courses and the ability to share your completion on LinkedIn, which is always a good resume-booster. 3.10 Credits Lion photo, HeatherBradleyPhotography Some rights reserved (used with author permission)A partial list of preattentive visual features, Christopher G. Healy Properties and Best Uses of Visual Encodings, Noah Iliinsky Some rights reserved Color Blind Essentials, by Colblindor 3.11 DataCamp Exercises The DataCamp exercises you are assigned primarily cover dplyr and ggplot2 in different ways. This breadth of coverage should help solidify your knowledge of these core packages which we’ll use throughout the semester. Both of these packages will also help clarify your thinking when manipulating data and visualizing it. This will make it easier to work with any data manipulation and visualization tools, including Excel. 3.12 What is R? Fun with R "],
["exploratory-data-analysis.html", "Week 4 Exploratory Data Analysis 4.1 Media 4.2 Media 4.3 joining data 4.4 Design Guidelines 4.5 geom selection 4.6 R Markdown 4.7 YAML options 4.8 Embedding plots 4.9 Prettier tables 4.10 Knitting 4.11 DataCamp Exercises 4.12 Assignment 01", " Week 4 Exploratory Data Analysis This unit spans Mon, Jan 27, 2020 through Sat, Feb 01, 2020. At 11:59 PM on Sat, Feb 01, 2020 the following items are due: Communicating with the Data in the Tidyverse or Introduction to Data Science in Python Now that we’ve learnt SQL and communication with RDBMS, we will cover how we extract data from SQL into R environment. We will learn some fundamental building blocks of R in the classroom. Besides it is time to introduce some packages as well which will make our life easier when manipulating and exploring data. We’ll also use external packages to visually explore our data (i.e., create simple visualizations). 4.1 Media Reading: For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights Reading: Markdown Reading: What is the tidyverse?, by Joseph Rickert Script: Script for EDA We’ll continue our exploration of the tidyverse with a specific focus on preparing reports containing data and visualizations for others. dplyr and ggplot2. In the notes, I’ll show you how to use the forcats package fct_reorder to change levels in graphs. In the videos, I’ll do the same using dplyr’s mutate function instead. There are often lots of ways to do the same thing in R. 4.2 Media Reading: What makes a chart boring?, Stephen Few Reading: Misleading graph, Wikipedia Video: dplyr joins Video: ggplot2 geoms library(babynames) library(tidyverse) 4.3 joining data Before we jump into communicating data, I want to cover a technique that is often used in analytics – joining data. We’ll use the babynames database to illustrate. I am a male born in 1967. You probably listened to people from my generation start out stories with “back in my day,” and usually expound on how much more difficult some aspect of life was. I’ll continue the tradition… Back in my day, people in the US weren’t as creative in naming their children. Apparently, they picked mostly New Testament, Christian names because that is what most other people were named in this country and there was potentially more pressure to conform. My name is the third most popular baby name from 1967 and The top five names made up nearly 20% of all the male names that year. top_five_males_1967 &lt;- babynames %&gt;% filter(sex == &quot;M&quot;, year == 1967) %&gt;% top_n(5) ## Selecting by prop top_five_males_1967 ## # A tibble: 5 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1967 M Michael 82445 0.0463 ## 2 1967 M David 66808 0.0375 ## 3 1967 M James 61699 0.0347 ## 4 1967 M John 61623 0.0346 ## 5 1967 M Robert 56380 0.0317 top_n also sends a message stating which variable it is using to determine the top n observations. Compare that to 2013, which is the most recent year from the babynames package. We see an influx of Old Testament names like Noah and Jacob, non-religious names like “Mason” and shortened names like “Liam.” Even more important is that the top five names combined don’t even account for five percent of all males born in 2013, which is one-quarter of the cumulative proportion of the top five names from 1967. Just by looking at that statistic, we can pretty much tell that there is likely far less conformity in naming male children in 2013 vs. 1967. top_five_males_2013 &lt;- babynames %&gt;% filter(sex == &quot;M&quot;, year == 2013) %&gt;% top_n(5) ## Selecting by prop top_five_males_2013 ## # A tibble: 5 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 M Noah 18241 0.00904 ## 2 2013 M Jacob 18148 0.00900 ## 3 2013 M Liam 18131 0.00899 ## 4 2013 M Mason 17688 0.00877 ## 5 2013 M William 16633 0.00825 Except for William, the top five names in 2013 were not at all popular in 1967. babynames %&gt;% filter(year %in% c(1967, 2013), sex == &quot;M&quot;, name %in% c(top_five_males_1967$name, top_five_males_2013$name)) ## # A tibble: 20 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1967 M Michael 82445 0.0463 ## 2 1967 M David 66808 0.0375 ## 3 1967 M James 61699 0.0347 ## 4 1967 M John 61623 0.0346 ## 5 1967 M Robert 56380 0.0317 ## 6 1967 M William 37621 0.0211 ## 7 1967 M Jacob 451 0.000253 ## 8 1967 M Noah 156 0.0000876 ## 9 1967 M Mason 87 0.0000489 ## 10 1967 M Liam 60 0.0000337 ## 11 2013 M Noah 18241 0.00904 ## 12 2013 M Jacob 18148 0.00900 ## 13 2013 M Liam 18131 0.00899 ## 14 2013 M Mason 17688 0.00877 ## 15 2013 M William 16633 0.00825 ## 16 2013 M Michael 15491 0.00768 ## 17 2013 M James 13552 0.00672 ## 18 2013 M David 12348 0.00612 ## 19 2013 M John 10704 0.00531 ## 20 2013 M Robert 6708 0.00333 You’ll notice I used the %in% clause to show the top five from each period’s popularity in 1967. Let’s assume I want to answer the following questions: what are the top five male names that appear in both 1967 and 2013? - to keep it simple I’ll use totals and not proportions what are the top five male names from 1967 that don’t appear in 2013? what are the top five male names from 2013 that don’t appear in 1967? 4.3.1 inner joins To answer the question: what are the top five names that appear in both 1967 and 2013?; I’m going first to create a data frame that joins the two periods. I’ll take multiple steps to illustrate, but I can make this syntactically simpler. inner_join creates a new data frame that “joins” the two objects by preferably some unique field that exists in both data frames, in this case, name. males_1967 &lt;- babynames %&gt;% filter(year == 1967, sex == &quot;M&quot;) males_2013 &lt;- babynames %&gt;% filter(year == 2013, sex == &quot;M&quot;) males_both &lt;- males_1967 %&gt;% inner_join(males_2013, by = &quot;name&quot;) head(males_both) ## # A tibble: 6 x 9 ## year.x sex.x name n.x prop.x year.y sex.y n.y prop.y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1967 M Michael 82445 0.0463 2013 M 15491 0.00768 ## 2 1967 M David 66808 0.0375 2013 M 12348 0.00612 ## 3 1967 M James 61699 0.0347 2013 M 13552 0.00672 ## 4 1967 M John 61623 0.0346 2013 M 10704 0.00531 ## 5 1967 M Robert 56380 0.0317 2013 M 6708 0.00333 ## 6 1967 M William 37621 0.0211 2013 M 16633 0.00825 You’ll notice that the “.x” represents 1967 data, and the “.y” represents 2013. I can add n.x and n.y to get a total, but I’m assuming things won’t change that much from 1967 due to the high concentration of names. males_both$n &lt;- males_both$n.x + males_both$n.y males_both %&gt;% select(name, n) %&gt;% top_n(5) ## Selecting by n ## # A tibble: 5 x 2 ## name n ## &lt;chr&gt; &lt;int&gt; ## 1 Michael 97936 ## 2 David 79156 ## 3 James 75251 ## 4 John 72327 ## 5 Robert 63088 Yep…the addition of the 2013 names didn’t even budge the order. The next question is more interesting… What are the top five male names from 1967 that don’t appear in 2013? We can’t use our joined data to answer this because that data frame explicitly contains names that only occur in both periods. To accomplish this, we need to do an outer join. 4.3.2 outer joins In dplyr, a left_join joins two tables using all the data from the “left” table and only matching data from the right table, so if I want to use all of the 1967 names, I make sure that table is syntactically to the left of left_join males_both_1967 &lt;- males_1967 %&gt;% left_join(males_2013, by = &quot;name&quot;) tail(males_both_1967) ## # A tibble: 6 x 9 ## year.x sex.x name n.x prop.x year.y sex.y n.y prop.y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1967 M Young 5 0.00000281 2013 M 8 0.00000397 ## 2 1967 M Zbigniew 5 0.00000281 NA &lt;NA&gt; NA NA ## 3 1967 M Zebedee 5 0.00000281 2013 M 10 0.00000496 ## 4 1967 M Zeno 5 0.00000281 2013 M 14 0.00000694 ## 5 1967 M Zenon 5 0.00000281 2013 M 12 0.00000595 ## 6 1967 M Zev 5 0.00000281 2013 M 160 0.0000793 Looking at the last few names alphabetically, we can the name Zbigniew was used in 1967 and not in 2013 (evident by the NA values in the &quot;*.y&quot; columns). So rewording the question in r-speak, what we are asking is: Show me the five highest n.x values for names where n.y is NA. males_both_1967 %&gt;% filter(is.na(n.y)) %&gt;% select (name, n.x) %&gt;% top_n(5) ## Selecting by n.x ## # A tibble: 5 x 2 ## name n.x ## &lt;chr&gt; &lt;int&gt; ## 1 Bart 534 ## 2 Tod 303 ## 3 Kraig 172 ## 4 Lon 162 ## 5 Kirt 155 We have uncovered a reverse-Simpsons-effect. Nobody in 2013 named their son Bart! To do the same for the 2013 data, we can either put the 2013 table to the left of the left_join, or the right of a right_join. males_both_2013 &lt;- males_1967 %&gt;% right_join(males_2013, by = &quot;name&quot;) males_both_2013 %&gt;% filter(is.na(n.x)) %&gt;% select (name, n.y) %&gt;% top_n(5) ## Selecting by n.y ## # A tibble: 5 x 2 ## name n.y ## &lt;chr&gt; &lt;int&gt; ## 1 Jayden 14756 ## 2 Aiden 13615 ## 3 Jaxon 7549 ## 4 Brayden 7438 ## 5 Ayden 6069 It looks like we can refer to 2013 as &quot;the rise of the *dens.&quot; It also appears that maybe my rush to label 2013 as “less conformist” may be wrong. We could have answered both of these questions doing a single full outer join as well. males_both_full &lt;- males_1967 %&gt;% full_join(males_2013, by = &quot;name&quot;) males_both_full %&gt;% filter(is.na(n.y)) %&gt;% select (name, n.x) %&gt;% top_n(5) ## Selecting by n.x ## # A tibble: 5 x 2 ## name n.x ## &lt;chr&gt; &lt;int&gt; ## 1 Bart 534 ## 2 Tod 303 ## 3 Kraig 172 ## 4 Lon 162 ## 5 Kirt 155 males_both_full %&gt;% filter(is.na(n.x)) %&gt;% select (name, n.y) %&gt;% top_n(5) ## Selecting by n.y ## # A tibble: 5 x 2 ## name n.y ## &lt;chr&gt; &lt;int&gt; ## 1 Jayden 14756 ## 2 Aiden 13615 ## 3 Jaxon 7549 ## 4 Brayden 7438 ## 5 Ayden 6069 4.4 Design Guidelines Let’s look at one of the visual encodings described in Iliinsky’s table – size, area. He has it listed as “Good” for quantitative values. If we compare this to Few’s use of “points of varying size,” we can see that Few only recommends this for geospatial data, specifically to pinpoint specific locations for entire regions. Part of this difference is due to the two people using different classification systems for visual encodings. “Size, area” is an expansive concept and you might think that there is some overlap between that category and Few’s categories of “Horizontal and Vertical Bars/Boxes.” For horizontal and vertical bars, it is the length that allows us to make the comparison so the “size, area” category doesn’t really overlap here. Horizontal and vertical box plots do have a slight area component to them but, once again, most of the preattentive processing is accomplished by the length and the line markers on the box plots. A common example of using size to encode quantitative information is the bubble chart (shown below). The chart shows what agencies the top 100 public servants in British Columbia worked in 2012. Size represents the count of public servants at that particular agency. I don’t think Iliinsky would be particularly fond of this chart. Stephen Few expresses his displeasure with it in a blog post. There probably is a nuanced difference between Few and Iliinsky on the applicability of bubble charts to static graphs which further illustrates the point that the visual encoding guidelines provided by both authors are suggestions, not law. For bar and column charts, you should only use points when the quantitative scale does not begin at zero. In general, you should have good reason not to have a zero start point as this can lead to a misleading graph. If we want to compare home sales for select Puget Sound counties, county is categorical (technically it is also geographic, but we aren’t mapping right now). We use the length of the bars to make comparisons. Kitsap county looks like it had about 2.4 times as many listings as Island county because the bar is roughly 2.4 times longer. Let’s assume that for some reason, my quantitative scale doesn’t begin with zero (note: you should be extremely suspicious when you see this). The chart below becomes exceptionally misleading because now it appears as though Kitsap county has over ten times the listings of Island County. You should never let this happen. If for some reason you are forced into using a non-zero start point (this sometimes happens in journalism), then you should use something that doesn’t force our brain into making comparisons via length or area. A dot plot is shown below, but the first bar chart is still the best option in this case. 4.5 geom selection We are going to look at Rolling Stone’s 500 greatest albums of all time (1955-2011) from cooldatasets.com. I’ve made a copy locally. top_albums &lt;- read.csv(url(&quot;http://jamessuleiman.com/teaching/datasets/Rolling_Stones_Top_500_Albums.csv&quot;), stringsAsFactors = FALSE) head(top_albums) ## Number Year Album Artist Genre ## 1 1 1967 Sgt. Pepper&#39;s Lonely Hearts Club Band The Beatles Rock ## 2 2 1966 Pet Sounds The Beach Boys Rock ## 3 3 1966 Revolver The Beatles Rock ## 4 4 1965 Highway 61 Revisited Bob Dylan Rock ## 5 5 1965 Rubber Soul The Beatles Rock, Pop ## 6 6 1971 What&#39;s Going On Marvin Gaye Funk / Soul ## Subgenre ## 1 Rock &amp; Roll, Psychedelic Rock ## 2 Pop Rock, Psychedelic Rock ## 3 Psychedelic Rock, Pop Rock ## 4 Folk Rock, Blues Rock ## 5 Pop Rock ## 6 Soul It looks like this dataset is mostly categorical data with Year and Number being exceptions. Let’s first see how the genres are distributed. If I don’t map a variable to the y axis, geom_bar will take discrete categorical variables and create a bin for each one and then provide a count for each discrete category. This can be explicitly specified by setting the attribute stat = bin but it is the default behavior for geom_bar. ggplot(data = top_albums, aes(x = factor(Genre))) + geom_bar() That is far more genres than I thought there would be. Let’s take a look at the top five genres. geom_bar’s default behavior is to place a count of data (i.e., stat = bin) on the y axis. In this case, we’ll use dplyr to pre-aggregate our data and we don’t want ggplot to attempt to count it. We switch to stat = identity to represent the value in the data frame rather than a count of occurrences. top_5_genres &lt;- top_albums %&gt;% group_by(Genre) %&gt;% summarize(count = n()) %&gt;% arrange(desc(count)) %&gt;% top_n(5) ggplot(data = top_5_genres, aes(x = Genre, y = count)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() You might have noticed that even though I created top_5_genres with a descending sort, ggplot2 doesn’t arrange the categories in this manner. We’ll take a look at top_5_genres first. str(top_5_genres) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 2 variables: ## $ Genre: chr &quot;Rock&quot; &quot;Funk / Soul&quot; &quot;Hip Hop&quot; &quot;Electronic, Rock&quot; ... ## $ count: int 249 38 29 19 18 Genre has not been defined as a factor and ggplot has no way to order it unless it is defined as a factor. We’ll use the fct_reorder function within mutate in the following manner: fct_reorder(categorical_variable, sorted_quantitative_variable_to_order_by) or, in other words, fct_reorder(Genre, count) ggplot(data = top_5_genres, aes(x = fct_reorder(Genre, count, .desc = TRUE), y = count)) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Genre&quot;) + coord_flip() You might be inclined also to encode Genre by color. I’m not a big fan of this in that the color would serve no useful purpose. You would be better off using color as an attribute here and not an aesthetic. ggplot(data = top_5_genres, aes(x = fct_reorder(Genre, count, .desc = TRUE), y = count)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;darkgreen&quot;) + xlab(&quot;Genre&quot;) + coord_flip() In the last couple of charts, we used geom_bar with stat = &quot;identity&quot; to help reinforce the difference between that and stat = &quot;bin&quot;. ggplot2 also has the geom geom_col that was explicitly designed for bar charts using values (i.e., the default is identity instead of bin). We could recreate the last chart in a slightly more straightforward manner as shown below. ggplot(data = top_5_genres, aes(x = fct_reorder(Genre, count, .desc = TRUE), y = count)) + geom_col(fill = &quot;darkgreen&quot;) + xlab(&quot;Genre&quot;) + coord_flip() Suppose we wanted to examine the count of top 500 records by year. Typically we show time series on the x-axis so we might do something like: ggplot(data = top_albums, aes(x = Year)) + geom_line(stat = &quot;bin&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Video did indeed kill the radio star. I had to specify stat = bin in this case because geom_line uses stat = identity by default. Because I didn’t create an aesthetic for y, there is no identity value. You’ll also notice I get a warning informing me that it is creating 30 bins, which ends up misrepresenting the data because I likely have more than 30 discrete years given that the data runs from 1955 - 2011. The suggestion to Pick better value with 'binwidth' is a great one. Setting bindwidth = 1 gives me a bin, and the corresponding count, for each year. Having fewer bins would tend to smooth the data. ggplot(data = top_albums, aes(x = Year)) + geom_line(stat = &quot;bin&quot;, binwidth = 1) top_5_artists &lt;- top_albums %&gt;% group_by(Artist) %&gt;% summarize(count = n()) %&gt;% top_n(5) %&gt;% arrange(desc(count)) %&gt;% ungroup %&gt;% inner_join(top_albums) ggplot(data = top_5_artists, aes(x = Year, y = fct_reorder(Artist, count), col = Genre)) + geom_point(size=5, alpha = 0.3) + geom_line(col = &quot;darkgreen&quot;) + ylab(&quot;Artist&quot;) For our purposes, it looks like the genres don’t add much to the story, and the legend takes up quite a bit of space. I also want to add a title. top_5_artists &lt;- top_albums %&gt;% group_by(Artist) %&gt;% summarize(count = n()) %&gt;% top_n(5) %&gt;% arrange(desc(count)) %&gt;% ungroup %&gt;% inner_join(top_albums) ggplot(data = top_5_artists, aes(x = Year, y = fct_reorder(Artist, count))) + geom_point(size=5, alpha = 0.3, col = &quot;red&quot;) + geom_line(col = &quot;darkgreen&quot;) + ggtitle(&quot;Most albums in the top 500 albums of all time (1955-2011)&quot;) + ylab(&quot;Artist&quot;) Feel free to review all of the geoms available in ggplot2. T 4.6 R Markdown Before we can explain R Markdown, we need to explain Markdown. Markdown is a straightforward markup language for formatting text that was developed by John Gruber in 2004. For example, if I enclose a word or phrase with underscores like this: _hello, I am italic_ it will render in italics as shown below. hello, I am italic Some other common markdown formats are shown below: **bold** `code` [hyperlinked text](url) ![optional caption text](path/to/img.png) # 1st level header ## 2nd level header * bullet item 1 * bullet item 2 …and others. R Markdown uses an extended form of markdown known as Pandoc Markdown. If you want to see a list of the available Pandoc markdown codes, in RStudio, select Help --&gt; Cheatsheets --&gt; R Markdown Cheat Sheet and look at the left side of the second page. R Markdown combines Pandoc Markdown formatting with the ability to display and run R code and show the output. R Markdown contains three essential components. an optional YAML header, enclosed with ___ R code chunks, enclosed with ``` Text formatting using Pandoc Markdown. In RStudio, select File --&gt; New File --&gt; R Markdown... and accept the default options and click OK. Take a look at the default contents of the new R Markdown file. Click Knit on the top of your editing window and notice that RStudio wants to name the document with a .Rmd extension. Go ahead and save the file and look at the knitted or rendered output. Notice the code, the output, and the visualization. Let’s take a look at some of the elements of the actual R Markdown file. The front-matter or YAML portion of the document is being used to specify the title of the document – in this case, “Untitled”, and the output format – in this case, HTML. Note that this portion is delimited by three hyphens ---. 4.6.1 code chunks The code chunks are delimited by three back-ticks ``` and, in this case, start with {r} which designates the language being used (we’ll only be using R in this course). The code in these sections runs and, by default, is displayed in the document and rendered to look different than regular text visually (i.e., it is usually in a grey box using a monospaced font). The code chunk options are settings that we use to control the behavior of the code chunk. More common options include include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and other chunks can use the results. echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures. results = &quot;hide&quot; displays code and not output. message = FALSE prevents messages that are generated by code from appearing in the finished file. warning = FALSE prevents warnings that are generated by code from appearing in the finished. The chunk name is optional, but it is good practice to name your chunks because it makes troubleshooting easier. If you want to look at some more chunk options, reference the R Markdown Reference Guide and see the knitr chunk options section (R Markdown uses the knitr library). 4.6.2 inline code We can also write code inline in our report without a code chunk by enclosing the inline code using &lt;backtick&gt; r &lt;code&gt; &lt;backtick&gt;. For example, one plus four equals 5. In the sentence above, I typed one plus four equals `r 1+4`. 4.7 YAML options There are a variety of formatting options specific to HTML documents in r markdown that are controlled through YAML options. Many of the options start with the output: html_document in the front matter as shown below. I’ll use some of the more common options and explain. --- title: &quot;My document&quot; author: &quot;Me&quot; output: html_document: toc: true toc_float: true number_sections: true code_folding: hide --- In the example above: toc: true specifies a table of contents (clickable links pointing to level 1 headers) toc_float: true has the table of contents floating to the left of the document (much like this course website). number_sections: true start table of contents numbering at one and increasing by one. code_folding: hide allows you not to display code and have a clickable show code button if the reader wants to see the code. There are many other options available, and you can read about them in the html_document_format publication by RStudio. 4.8 Embedding plots To date, we haven’t really created visualizations in R. There are many packages that extend the base graphics available to us in R. We will be using the ggvis package for most of this semester – it will also be prominently used in the visualization course. Embedding plots in R Markdown is pretty straightforward but there are a couple of things we need to consider: What format is my final report going to be output to (e.g., html, pdf, Word, ioslides, etc.). How are my reader’s going to consume my report (desktop/laptop, mobile device, printed) For this unit, we’ll just be working with html because it is the most dynamic form of output. For figures, we might want to set the fig.width and fig.height. If we don’t, they will default to seven (measurements are in inches). In the image below, I have fig.width=6 and fig.height=8. library(tidyverse) mtcars %&gt;% ggplot(aes(wt, mpg)) + geom_point() 4.9 Prettier tables There are some decent formatting options for tables in R Markdown. Below are three different versions of the same table: Default output &lt;- head(mtcars %&gt;% select(wt, mpg), 10) output ## wt mpg ## Mazda RX4 2.620 21.0 ## Mazda RX4 Wag 2.875 21.0 ## Datsun 710 2.320 22.8 ## Hornet 4 Drive 3.215 21.4 ## Hornet Sportabout 3.440 18.7 ## Valiant 3.460 18.1 ## Duster 360 3.570 14.3 ## Merc 240D 3.190 24.4 ## Merc 230 3.150 22.8 ## Merc 280 3.440 19.2 kable library(knitr) kable(output, digits = 2) wt mpg Mazda RX4 2.62 21.0 Mazda RX4 Wag 2.88 21.0 Datsun 710 2.32 22.8 Hornet 4 Drive 3.21 21.4 Hornet Sportabout 3.44 18.7 Valiant 3.46 18.1 Duster 360 3.57 14.3 Merc 240D 3.19 24.4 Merc 230 3.15 22.8 Merc 280 3.44 19.2 pander library(pander) panderOptions(&#39;round&#39;, 2) set.caption(&quot;mtcars weight and mpg&quot;) pander(output) mtcars weight and mpg wt mpg Mazda RX4 2.62 21 Mazda RX4 Wag 2.88 21 Datsun 710 2.32 22.8 Hornet 4 Drive 3.21 21.4 Hornet Sportabout 3.44 18.7 Valiant 3.46 18.1 Duster 360 3.57 14.3 Merc 240D 3.19 24.4 Merc 230 3.15 22.8 Merc 280 3.44 19.2 There are several other table formatting packages that we aren’t going to cover but to summarize: default tables aren’t too pretty kable is part of knitr and is a simple way to make prettier tables pander has more options than kable, but is more complex there are a variety of other packages that might suit your specific need (e.g., xtable, htmltables, etc.) but for this class pander and kable should have you covered. 4.10 Knitting At any time, you can knit your R Markdown file. In RStudio, you can use the Knit or KnitHTML button to specify if you want to knit to html, pdf, or a Microsoft Word document. What actually happens behind the scenes is that RStudio uses the rmarkdown package to render the output in your specified format. The video will have more detailed use of R Markdown. 4.11 DataCamp Exercises The DataCamp exercises are focused on creating visualizations for others. When we develop visualizations for ourselves, we are often creating them to explore data. When we build them for others, we are creating them to communicate. 4.12 Assignment 01 Solution: A01 "],
["multiple-linear-regression.html", "Week 5 Multiple Linear Regression 5.1 Media 5.2 Regression using R", " Week 5 Multiple Linear Regression This unit spans Mon, Feb 03, 2020 through Sat, Feb 08, 2020. At 11:59 PM on Sat, Feb 8, 2020 the following items are due: Correlation and Regression in R or Introduction to Linear Modeling in Python Now that we’ve learnt how data moves from data store to visual dashboard. How data is transformed at different stages. Now we will focus on data analysis techniques. In this week, we focus on Mulitple linear regression. We will continue to follow the same steps that we followed earlier to make the data analysis available publicly. 5.1 Media Lecture: Data Analysis Framework Video: Simple Linear Regression Video: Multiple Linear Regression 5.2 Regression using R PowerPoint: Statistical Analysis using R R Script: Statistical Analysis using R "],
["statistics-canada.html", "Week 6 Statistics Canada 6.1 Media 6.2 Guidelines and Evaluation", " Week 6 Statistics Canada This unit spans Mon, Feb 10, 2020 through Sat, Feb 15, 2020. At 11:59 PM on Sat, Feb 15, 2020 the following items are due: Multiple and Logistic Regression in R or Generalized Linear Models in Python This unit focuses on the approach to the Data Science competition. This page will be updated as we gather more information and knowledge about the challenge. 6.1 Media Reading: Business Data Scientist Challenge - Predict GDP Reading: Why GDP is so Difficult to Measure? Reference: Leading and Lagging Economic Indicators 6.2 Guidelines and Evaluation The group assignment shall be evaluated using the following four components. Evaluation by other group member (5 points): Each group member shall rate the other group member on a 5 point scale. EDA &amp; Feature Engineering (5 points): The exploratory data analysis and features engineering shall be weight for 5 points. Final set of features is required to be submitted on D2l by March 14. Model Selection and Building (5 points): Each group will select an appropriate model and fine tune model parameters to determine GDP. Initially students will apply linear regression based models and subsequently use non-parametric models as deemed fit. Model is required to be submitted on D2L portal by March 21. Model Interpretation and Final Report (5 points): Each group will submit a final report of their modeling exercise. It can either be a Rscript file with associated a PPT or a Rmarkdown file or an interactive ipynb file. The final report is required to be submitted by March 28. Any group shortlisted for final round on Data Science competition will automatically be awarded 20 points over riding any previous evaluations. "],
["logit-and-lda.html", "Week 7 Logit and LDA 7.1 Media 7.2 PowerPoint Presentation", " Week 7 Logit and LDA This unit spans Mon, Feb 17, 2020 through Sat, Feb 22, 2020. At 11:59 PM on Sat, Feb 22, 2020 the following items are due: Data Visualization with ggplot2 or Data Visualization with Seaborn In this unit we discuss various models 7.1 Media Lecture Slides: Regression: : Linear Logistic and LDA Reading: Log Odds Reading: Linear Discriminant Analysis Reading: Logistics Regression Video: Logistics Regression 7.2 PowerPoint Presentation PowerPoint: Logit and LDA "],
["pca-and-clustering.html", "Week 8 PCA and Clustering 8.1 Media 8.2 PCA and Clustering", " Week 8 PCA and Clustering This unit spans Mon, Feb 24, 2020 through Sat, Feb 29, 2020. At 11:59 PM on Sat, Feb 29, 2020 the following items are due: Feature Engineering with R or Python In this unit we will focus on feature engineering using PCA and clustering. 8.1 Media Lecture: Cluster and Factor Analysis Reference: Hierarchical clustering Video: EDA Wickham Hadley Way Video Playlist: How to win Data Science Competition Pick the videos you find most relevant to you. 8.2 PCA and Clustering PowerPoint: PCA and Clustering "],
["model-selection.html", "Week 9 Model Selection 9.1 Media", " Week 9 Model Selection This unit spans Mon, Mar 09, 2020 through Sat, Mar 14, 2020. At 11:59 PM on Sat, Mar 14, 2020 the following items are due: Dimensionality Reduction in R or Python Model Selection and Regularization 9.1 Media Lecture: Model Selection and Evaluation Lecture: Selecting Predictor variables Lecture: Shrinkage / Regularization PowerPoint: Model Selection and Regularization "],
["tree-based-models.html", "Week 10 Tree Based Models 10.1 Media", " Week 10 Tree Based Models This unit spans Mon, Mar 16, 2020 through Sat, Mar 21, 2020. At 11:59 PM on Sat, Mar 21, 2020 the following items are due: Cluster Analysis in R or SciPy Tree based classification models 10.1 Media "],
["random-forest.html", "Week 11 Random Forest 11.1 Media", " Week 11 Random Forest This unit spans Mon, Mar 23, 2020 through Sat, Mar 28, 2020. At 11:59 PM on Sat, Mar 28, 2020 the following items are due: Machine Learning with Tree-Based Models in R or Python Random Forest 11.1 Media "],
["support-vector-machine.html", "Week 12 Support Vector Machine 12.1 Media", " Week 12 Support Vector Machine This unit spans Mon, Mar 30, 2020 through Sat, Apr 04, 2020. At 11:59 PM on Sat, Apr 04, 2020 the following items are due: Support Vector Machine in R or Linear Classifiers in Python Support Vector Machines 12.1 Media "],
["course-summary.html", "Week 13 Course Summary 13.1 Media", " Week 13 Course Summary This unit spans Mon, Apr 06, 2020 through Thu, Apr 09, 2020. There are no items due this unit. Course Summary 13.1 Media "]
]
